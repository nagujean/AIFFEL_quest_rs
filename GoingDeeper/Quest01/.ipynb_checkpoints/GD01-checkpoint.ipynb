{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7166dcf2-37a6-4c22-9c77-e052fa6edd20",
   "metadata": {},
   "source": [
    "#GoingDeeper ë©‹ì§„ ì‚¬ì „ ë§Œë“¤ê¸° í”„ë¡œì íŠ¸  \n",
    "\n",
    "1. í”„ë¡œì íŠ¸ ëª©í‘œ   \n",
    "    1.1ëª©í‘œ  \n",
    "        -í…ìŠ¤íŠ¸ ì²˜ë¦¬ì™€ í† í¬ë‚˜ì´ì € ì´í•´  \n",
    "      \n",
    "    1.2 í…ŒìŠ¤í¬  \n",
    "        -ì½”í¼ìŠ¤ ë¶„ì„, ì „ì²˜ë¦¬, SentencePiece ì ìš©, í† í¬ë‚˜ì´ì € êµ¬í˜„ ë° ë™ì‘ ì§„í–‰.  \n",
    "        -SentencePiece í† í¬ë‚˜ì´ì €ê°€ ì ìš©ëœ Text Classifier ëª¨ë¸ì´ ì •ìƒì ìœ¼ë¡œ ìˆ˜ë ´í•˜ì—¬ 80% ì´ìƒì˜ test accuracyê°€ í™•ì¸.  \n",
    "        -SentencePiece ì™€ ë‹¤ë¥¸ í† í¬ë‚˜ì´ì € í˜¹ì€ SentencePieceì˜ ë‹¤ë¥¸ ì˜µì…˜ì˜ ê²½ìš°ì™€ ë¹„êµ ë¶„ì„  \n",
    "\n",
    "\n",
    "  \n",
    "2. Baseline ëª¨ë¸ì€ ì„ íƒ - GRU   \n",
    "    ì„ íƒí•œ ì´ìœ ëŠ” GRUëŠ”  LSTMì˜ ë¯¸ë‹ˆë©€ ë²„ì „ìœ¼ë¡œ ì†ë„ê°€ ë¹ ë¥´ë‹¤.  \n",
    "    ë³¸ í”„ë¡œì íŠ¸ì—ì„œëŠ” RNN ê³„ì—´ ë‚´ì—ì„œ í† í¬ë‚˜ì´ì € ì„±ëŠ¥ ì‹¤í—˜ì´ í•µì‹¬ìœ¼ë¡œ ìš”êµ¬ëœë‹¤.  \n",
    "    ë”°ë¼ì„œ RNN ê³„ì—´ì´ë©´ì„œ ë¹ ë¥¸ ì‹¤í—˜ì´ ê°€ëŠ¥í•œ GRUë¥¼ ì„ íƒí•˜ì˜€ë‹¤.   \n",
    "  \n",
    "3. EDA ê²°ê³¼  (ë³„ë„ íŒŒì¼ GD01_EDA.ipynbì°¸ê³ )  \n",
    "    -: 'ì¢‹ì€', 'ì—†ë‹¤'ì™€ ê°™ì€ ë‹¨ì–´ë‚˜ 'ã…‹ã…‹', 'ã… ã… ' ê°™ì€ ê¸°í˜¸ë“¤ì€ ê¸ì • ë¶€ì •ì—ì„œ ëª¨ë‘ ì‚¬ìš© ë¹ˆë„ê°€ ë†’ì€ ì¤‘ì˜ì  í‘œí˜„ì„. ì´ëŸ° ë‹¨ì–´ ë‹¨ë…ìœ¼ë¡œ ê°ì •ì„ íŠ¹ì •í•˜ê¸° ì–´ë ¤ìš¸ ìˆ˜ ìˆìŒ  \n",
    "    -'ìµœê³ ', 'ìµœì•…', 'ì•„ê¹ë‹¤'ì™€ ê°™ì€ í˜•ìš©ì‚¬ë‚˜ 'ã…ã…'(ê¸ì •), 'ã…¡ã…¡'(ë¶€ì •) ê°™ì€ íŠ¹ì • ê¸°í˜¸ë“¤ì€ ë§¤ìš° ëª…í™•í•œ ê¸ì •/ë¶€ì • ì‹ í˜¸ë¡œ ë³´ì„  \n",
    "    -N-gram ë¶„ì„ì‹œ \"ì¬ë¯¸ë„ ì—†ê³ \"(ë¶€ì •)ì™€ \"í ì¡ì„ ë° ì—†ëŠ”\"(ê¸ì •)ì²˜ëŸ¼ í•¨ê»˜ ì“°ì´ëŠ” ë‹¨ì–´ë¥¼ í†µí•´ ê¸ë¶€ì • ì‹ í˜¸ê°€ ëª…í™•í•˜ê²Œ íŒë‹¨ë¨  \n",
    "  \n",
    "4. ì‹¤í—˜ ê¸°ë¡  \n",
    "    1) ë² ì´ìŠ¤ ë¼ì¸ êµ¬ì¶• : Sentencepece í† í¬ë‚˜ì´ì €, ëª¨ë¸ GRU  \n",
    "    2) í† í¬ë‚˜ì´ì €ë³„ vocab_size ì‹¤í—˜  \n",
    "        -SentencePiece í† í¬ë‚˜ì´ì € vocab_size ì‹¤í—˜ : 10000, 15000, 20000, 30000  \n",
    "        -KoNLPy(Mecab) í† í¬ë‚˜ì´ì € vocab_size ì‹¤í—˜ : 10000, 15000, 20000, 30000  \n",
    "    3) ì¤‘ì˜ì  ë‹¨ì–´ ë¶ˆìš©ì–´ ì¶”ê°€ ì‹¤í—˜  \n",
    "        -SentencePiece í† í¬ë‚˜ì´ì € ë¶ˆìš©ì–´ ì¶”ê°€  \n",
    "        -KoNLPy(Mecab) í† í¬ë‚˜ì´ì € ë¶ˆìš©ì–´ ì¶”ê°€  \n",
    "    4) í† í¬ë‚˜ì´ì € ëª¨ë¸ ë³€ê²½  \n",
    "        -Sentencepiece í† í¬ë‚˜ì´ì € ëª¨ë¸ ë³€ê²½ bpe â†’ unigram  \n",
    "    5) SentencePieceí† í¬ë‚˜ì´ì € ì˜µì…˜ ë³€ê²½ ì‹¤í—˜  \n",
    "        -character_coverage,user_defined_symbols ë³€ê²½  \n",
    "    6) MAX_LEN ë³€ê²½ ì‹¤í—˜ 40 â†’ 110  \n",
    "    7) KoNLPy (Mecab) í† í¬ë‚˜ì´ì € í’ˆì‚¬(POS) ê¸°ë°˜ ë‹¨ì–´ í•„í„°ë§ ì¶”ê°€ ì‹¤í—˜  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555866b1-7c9c-4605-8ce5-21d5070842fe",
   "metadata": {},
   "outputs": [],
   "source": [
    " # ì½”ë“œ\n",
    "\n",
    "# ====================================================================================\n",
    "# 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ Import\n",
    "# ====================================================================================\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# --- ë¹„êµ ì‹¤í—˜ì„ ìœ„í•œ í† í¬ë‚˜ì´ì € ---\n",
    "import sentencepiece as spm\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "# ====================================================================================\n",
    "# 2. ì¤‘ì•™ ì„¤ì • (Configuration)\n",
    "# ====================================================================================\n",
    "CFG = {\n",
    "    # --- ì‹¤í—˜ ì„ íƒ ---\n",
    "    'TOKENIZER_TYPE': 'SentencePiece', # 'SentencePiece' ë˜ëŠ” 'KoNLPy' ì„ íƒ\n",
    "    'KONLPY_TOKENIZER': 'Mecab',     # TOKENIZER_TYPEì´ 'KoNLPy'ì¼ ê²½ìš°, ì‚¬ìš©í•  í˜•íƒœì†Œ ë¶„ì„ê¸°\n",
    "    \n",
    "    # --- SentencePiece í•˜ì´í¼íŒŒë¼ë¯¸í„° ---\n",
    "    'SP_VOCAB_SIZE': 10000,          # SentencePiece ë‹¨ì–´ ì§‘í•© í¬ê¸°\n",
    "    'SP_MODEL_TYPE': 'bpe',          # 'bpe' ë˜ëŠ” 'unigram'\n",
    "    \n",
    "    # --- KoNLPy í•˜ì´í¼íŒŒë¼ë¯¸í„° (ì¶”ê°€) ---\n",
    "    'KONLPY_VOCAB_SIZE': 10000,      # KoNLPy ì‚¬ìš© ì‹œ ë‹¨ì–´ ì§‘í•© í¬ê¸° ì œí•œ\n",
    "    \n",
    "    # --- ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ---\n",
    "    'EMBEDDING_DIM': 128,\n",
    "    'HIDDEN_DIM': 128,\n",
    "    'N_LAYERS': 1,\n",
    "    'DROPOUT': 0.6,\n",
    "    \n",
    "    # --- í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„° ---\n",
    "    'EPOCHS': 20,\n",
    "    'LEARNING_RATE': 0.001,\n",
    "    'BATCH_SIZE': 128,\n",
    "    'MAX_LEN': 40,                   # ë¬¸ì¥ ìµœëŒ€ ê¸¸ì´\n",
    "    'PATIENCE': 3,                   # ì¡°ê¸° ì¢…ë£Œ ì¡°ê±´\n",
    "    'WEIGHT_DECAY': 1e-5,            # ê°€ì¤‘ì¹˜ ê°ì‡  (L2 ê·œì œ)\n",
    "    \n",
    "    # --- ê¸°íƒ€ ì„¤ì • ---\n",
    "    'SEED': 42,\n",
    "    'DEVICE': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    'MODEL_SAVE_PATH': 'best_model.pth',\n",
    "}\n",
    "\n",
    "# ====================================================================================\n",
    "# 3. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜\n",
    "# ====================================================================================\n",
    "def seed_everything(seed):\n",
    "    \"\"\"ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ê³ ì • í•¨ìˆ˜\"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# ====================================================================================\n",
    "# 4. ë°ì´í„° ì¤€ë¹„ í•¨ìˆ˜\n",
    "# ====================================================================================\n",
    "def load_and_preprocess_data():\n",
    "    \"\"\"NSMC ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ì „ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜\"\"\"\n",
    "    data_path = os.path.join(os.getenv(\"HOME\"), 'work', 'workplace', 'AIFFEL_quest_rs', 'Exploration', 'Quest05', 'sentiment_classification', 'data')\n",
    "    train_data = pd.read_table(os.path.join(data_path, 'ratings_train.txt'))\n",
    "    test_data = pd.read_table(os.path.join(data_path, 'ratings_test.txt'))\n",
    "    \n",
    "    # --- í›ˆë ¨ ë°ì´í„° ì •ì œ ---\n",
    "    train_data.dropna(subset=['document'], inplace=True)\n",
    "    train_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "    \n",
    "    # --- í…ŒìŠ¤íŠ¸ ë°ì´í„° ì •ì œ ---\n",
    "    test_data.dropna(subset=['document'], inplace=True)\n",
    "    test_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "    \n",
    "    # --- ë°ì´í„°ì…‹ ë¶„ë¦¬ ---\n",
    "    train_set, val_set = train_test_split(train_data, test_size=0.2, random_state=CFG['SEED'], stratify=train_data['label'])\n",
    "    \n",
    "    print(\"âœ… ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ ì™„ë£Œ\")\n",
    "    return train_set, val_set, test_data\n",
    "\n",
    "# ====================================================================================\n",
    "# 5. í† í¬ë‚˜ì´ì € ìƒì„± í•¨ìˆ˜ (KoNLPy ë¡œì§ ìˆ˜ì •)\n",
    "# ====================================================================================\n",
    "def get_tokenizer(cfg, train_df):\n",
    "    \"\"\"CFGì— ë”°ë¼ SentencePiece ë˜ëŠ” KoNLPy í† í¬ë‚˜ì´ì €ì™€ vocab_sizeë¥¼ ë°˜í™˜\"\"\"\n",
    "    \n",
    "    if cfg['TOKENIZER_TYPE'] == 'SentencePiece':\n",
    "        # --- (SentencePiece ë¡œì§ì€ ì´ì „ê³¼ ë™ì¼) ---\n",
    "        corpus_path = 'nsmc_corpus.txt'\n",
    "        model_prefix = f'nsmc_{cfg[\"SP_MODEL_TYPE\"]}_{cfg[\"SP_VOCAB_SIZE\"]}'\n",
    "        train_df['document'].to_csv(corpus_path, index=False, header=False)\n",
    "        spm.SentencePieceTrainer.train(\n",
    "            f'--input={corpus_path} --model_prefix={model_prefix} '\n",
    "            f'--vocab_size={cfg[\"SP_VOCAB_SIZE\"]} --model_type={cfg[\"SP_MODEL_TYPE\"]}'\n",
    "        )\n",
    "        processor = spm.SentencePieceProcessor()\n",
    "        processor.load(f'{model_prefix}.model')\n",
    "        vocab_size = processor.get_piece_size()\n",
    "\n",
    "        def tokenize_fn(corpus, max_len):\n",
    "            sequences = []\n",
    "            for sentence in corpus:\n",
    "                ids = processor.encode_as_ids(str(sentence))\n",
    "                ids = ids[:max_len] if len(ids) > max_len else ids + [0] * (max_len - len(ids))\n",
    "                sequences.append(ids)\n",
    "            return torch.tensor(sequences, dtype=torch.long)\n",
    "            \n",
    "        print(f\"âœ… SentencePiece í† í¬ë‚˜ì´ì € ì¤€ë¹„ ì™„ë£Œ (vocab_size: {vocab_size})\")\n",
    "        return tokenize_fn, vocab_size\n",
    "\n",
    "    elif cfg['TOKENIZER_TYPE'] == 'KoNLPy':\n",
    "        # --- KoNLPy í† í¬ë‚˜ì´ì € ìƒì„± (vocab_size ì œí•œ ë¡œì§ ì¶”ê°€) ---\n",
    "        if cfg['KONLPY_TOKENIZER'] == 'Mecab':\n",
    "            # tokenizer = Mecab()\n",
    "            # ê¸°ì¡´ ì½”ë“œ: tokenizer = Mecab()\n",
    "            # ìˆ˜ì •í•  ì½”ë“œ:\n",
    "            tokenizer = Mecab('/usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ko-dic')\n",
    "\n",
    "        else:\n",
    "            from konlpy.tag import Okt\n",
    "            tokenizer = Okt()\n",
    "            \n",
    "        # 1. ë‹¨ì–´ ë¹ˆë„ìˆ˜ ê³„ì‚°\n",
    "        word_counts = {}\n",
    "        for sentence in tqdm(train_df['document'], desc=\"KoNLPy Freq. Counting\"):\n",
    "            tokens = tokenizer.morphs(str(sentence))\n",
    "            for token in tokens:\n",
    "                word_counts[token] = word_counts.get(token, 0) + 1\n",
    "        \n",
    "        # 2. ë¹ˆë„ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ìƒìœ„ ë‹¨ì–´ ì„ íƒ\n",
    "        # <PAD>, <UNK> í† í°ì„ ìœ„í•´ (vocab_size - 2)ê°œë§Œ ì„ íƒ\n",
    "        sorted_words = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "        top_words = sorted_words[:cfg['KONLPY_VOCAB_SIZE'] - 2]\n",
    "        \n",
    "        # 3. ìµœì¢… ë‹¨ì–´ ì‚¬ì „ êµ¬ì¶•\n",
    "        word_index = {'<PAD>': 0, '<UNK>': 1}\n",
    "        for word in top_words:\n",
    "            word_index[word] = len(word_index)\n",
    "            \n",
    "        vocab_size = len(word_index)\n",
    "\n",
    "        def tokenize_fn(corpus, max_len):\n",
    "            sequences = []\n",
    "            for sentence in corpus:\n",
    "                tokens = tokenizer.morphs(str(sentence))\n",
    "                ids = [word_index.get(token, 1) for token in tokens] # ì‚¬ì „ì— ì—†ìœ¼ë©´ <UNK> (1)\n",
    "                ids = ids[:max_len] if len(ids) > max_len else ids + [0] * (max_len - len(ids))\n",
    "                sequences.append(ids)\n",
    "            return torch.tensor(sequences, dtype=torch.long)\n",
    "            \n",
    "        print(f\"âœ… KoNLPy({cfg['KONLPY_TOKENIZER']}) í† í¬ë‚˜ì´ì € ì¤€ë¹„ ì™„ë£Œ (vocab_size: {vocab_size})\")\n",
    "        return tokenize_fn, vocab_size\n",
    "\n",
    "# ====================================================================================\n",
    "# 6. ëª¨ë¸ ì •ì˜\n",
    "# ====================================================================================\n",
    "class SentimentGRU(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers, dropout):\n",
    "        super(SentimentGRU, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers=n_layers, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        gru_out, hidden = self.gru(embedded)\n",
    "        last_output = gru_out[:, -1, :]\n",
    "        last_output = self.dropout(last_output)\n",
    "        output = self.fc(last_output)\n",
    "        return output\n",
    "\n",
    "# ====================================================================================\n",
    "# 7. í•™ìŠµ ë° í‰ê°€ í•¨ìˆ˜ (ì¶œë ¥ê°’ ìˆ˜ì •)\n",
    "# ====================================================================================\n",
    "def run_experiment(cfg, train_set, val_set, test_set):\n",
    "    \"\"\"í•˜ë‚˜ì˜ ì„¤ì •(CFG)ìœ¼ë¡œ ì „ì²´ ì‹¤í—˜ì„ ì‹¤í–‰í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜\"\"\"\n",
    "    \n",
    "    # --- 1. í† í¬ë‚˜ì´ì € ë° ë°ì´í„°ë¡œë” ì¤€ë¹„ ---\n",
    "    tokenize_fn, vocab_size = get_tokenizer(cfg, train_set)\n",
    "    \n",
    "    X_train = tokenize_fn(train_set['document'].tolist(), cfg['MAX_LEN'])\n",
    "    y_train = torch.tensor(train_set['label'].values, dtype=torch.float32)\n",
    "    X_val = tokenize_fn(val_set['document'].tolist(), cfg['MAX_LEN'])\n",
    "    y_val = torch.tensor(val_set['label'].values, dtype=torch.float32)\n",
    "    X_test = tokenize_fn(test_set['document'].tolist(), cfg['MAX_LEN'])\n",
    "    y_test = torch.tensor(test_set['label'].values, dtype=torch.float32)\n",
    "\n",
    "    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=cfg['BATCH_SIZE'], shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=cfg['BATCH_SIZE'], shuffle=False)\n",
    "    test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=cfg['BATCH_SIZE'], shuffle=False)\n",
    "    \n",
    "    # --- 2. ëª¨ë¸, ì†ì‹¤í•¨ìˆ˜, ì˜µí‹°ë§ˆì´ì € ì •ì˜ ---\n",
    "    model = SentimentGRU(vocab_size, cfg['EMBEDDING_DIM'], cfg['HIDDEN_DIM'], cfg['N_LAYERS'], cfg['DROPOUT']).to(cfg['DEVICE'])\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=cfg['LEARNING_RATE'], weight_decay=cfg['WEIGHT_DECAY'])\n",
    "    \n",
    "    # --- 3. í•™ìŠµ ë° ì¡°ê¸° ì¢…ë£Œ ---\n",
    "    patience_counter = 0\n",
    "    best_loss = np.Inf\n",
    "    \n",
    "    print(\"\\nğŸš€ ëª¨ë¸ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
    "    for epoch in range(cfg['EPOCHS']):\n",
    "        # --- í›ˆë ¨ ë‹¨ê³„ ---\n",
    "        model.train()\n",
    "        train_loss, train_correct, train_total = 0, 0, 0\n",
    "        for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1:02d} [Train]\"):\n",
    "            inputs, labels = inputs.to(cfg['DEVICE']), labels.to(cfg['DEVICE'])\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs.squeeze(), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            preds = torch.sigmoid(outputs.squeeze()) > 0.5\n",
    "            train_correct += (preds == labels).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "\n",
    "        # <<<--- í›ˆë ¨ ê²°ê³¼ ê³„ì‚° --- START ---\n",
    "        # ë§¤ ì—í¬í¬ì˜ í›ˆë ¨ì´ ëë‚˜ë©´ í‰ê·  ì†ì‹¤ê³¼ ì •í™•ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_accuracy = train_correct / train_total\n",
    "        # <<<--- í›ˆë ¨ ê²°ê³¼ ê³„ì‚° --- END ---\n",
    "        \n",
    "        # --- ê²€ì¦ ë‹¨ê³„ ---\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(cfg['DEVICE']), labels.to(cfg['DEVICE'])\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_fn(outputs.squeeze(), labels)\n",
    "                val_loss += loss.item()\n",
    "                preds = torch.sigmoid(outputs.squeeze()) > 0.5\n",
    "                val_correct += (preds == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = val_correct / val_total\n",
    "\n",
    "        # <<<--- ì¶œë ¥ë¬¸ ìˆ˜ì • --- START ---\n",
    "        # ê¸°ì¡´ ì¶œë ¥ë¬¸ì— avg_train_lossì™€ train_accuracyë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.\n",
    "        print(f\"Epoch {epoch+1:02d} | \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f} | Train Acc: {train_accuracy*100:.2f}% | \"\n",
    "              f\"Val Loss: {avg_val_loss:.4f} | Val Acc: {val_accuracy*100:.2f}%\")\n",
    "        # <<<--- ì¶œë ¥ë¬¸ ìˆ˜ì • --- END ---\n",
    "        \n",
    "        if avg_val_loss < best_loss:\n",
    "            best_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), cfg['MODEL_SAVE_PATH'])\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= cfg['PATIENCE']:\n",
    "            print(f\"ğŸ›‘ Early stopping triggered after {epoch+1} epochs.\")\n",
    "            break\n",
    "            \n",
    "    # --- 4. ìµœì¢… í‰ê°€ ---\n",
    "    print(f\"\\nğŸ§ª ìµœê³  ì„±ëŠ¥ ëª¨ë¸('{cfg['MODEL_SAVE_PATH']}')ë¡œ ìµœì¢… í‰ê°€ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
    "    model.load_state_dict(torch.load(cfg['MODEL_SAVE_PATH']))\n",
    "    model.eval()\n",
    "    test_loss, test_correct, test_total = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(test_loader, desc=\"[Testing]\"):\n",
    "            inputs, labels = inputs.to(cfg['DEVICE']), labels.to(cfg['DEVICE'])\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs.squeeze(), labels)\n",
    "            test_loss += loss.item()\n",
    "            preds = torch.sigmoid(outputs.squeeze()) > 0.5\n",
    "            test_correct += (preds == labels).sum().item()\n",
    "            test_total += labels.size(0)\n",
    "\n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    test_accuracy = test_correct / test_total\n",
    "    \n",
    "    print(\"\\nğŸ‰ ëª¨ë¸ í‰ê°€ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"  - ìµœì¢… í…ŒìŠ¤íŠ¸ ì†ì‹¤ (Loss): {avg_test_loss:.4f}\")\n",
    "    print(f\"  - ìµœì¢… í…ŒìŠ¤íŠ¸ ì •í™•ë„ (Accuracy): {test_accuracy*100:.2f}%\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    return {'loss': avg_test_loss, 'accuracy': test_accuracy}\n",
    "\n",
    "# ====================================================================================\n",
    "# 8. ë©”ì¸ ì‹¤í–‰ ë¸”ë¡\n",
    "# ====================================================================================\n",
    "if __name__ == '__main__':\n",
    "    seed_everything(CFG['SEED'])\n",
    "    train_set, val_set, test_set = load_and_preprocess_data()\n",
    "    \n",
    "    # --- ì—¬ê¸°ì„œë¶€í„° ì‹¤í—˜ì„ ì‹¤í–‰í•©ë‹ˆë‹¤ ---\n",
    "    # ì˜ˆì‹œ 1: SentencePiece (bpe, vocab_size=10000) ì‹¤í—˜\n",
    "    results = run_experiment(CFG, train_set, val_set, test_set)\n",
    "    \n",
    "    # ì˜ˆì‹œ 2: KoNLPy (Mecab) ì‹¤í—˜ (CFG ë³€ê²½ í›„ ì‹¤í–‰)\n",
    "    # CFG['TOKENIZER_TYPE'] = 'KoNLPy'\n",
    "    # CFG['KONLPY_TOKENIZER'] = 'Mecab'\n",
    "    # results_mecab = run_experiment(CFG, train_set, val_set, test_set)\n",
    "    \n",
    "    # ì˜ˆì‹œ 3: SentencePiece (unigram, vocab_size=16000) ì‹¤í—˜ (CFG ë³€ê²½ í›„ ì‹¤í–‰)\n",
    "    # CFG['TOKENIZER_TYPE'] = 'SentencePiece'\n",
    "    # CFG['SP_MODEL_TYPE'] = 'unigram'\n",
    "    # CFG['SP_VOCAB_SIZE'] = 16000\n",
    "    # results_sp_uni = run_experiment(CFG, train_set, val_set, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06b96ad-cf73-4de7-9145-e0d869c3fe93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87df8fbe-810b-4f3a-98f6-529e8afc96ba",
   "metadata": {},
   "source": [
    "# ì‹¤í—˜ ê²°ê³¼"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70304a55-5885-4036-86b8-e1e5ab699c8a",
   "metadata": {},
   "source": [
    "# í† í¬ë‚˜ì´ì €ë³„ Vocab Size ë³€ê²½ ì‹¤í—˜"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9f6b4e-c320-466c-b9de-98ab9400beb4",
   "metadata": {},
   "source": [
    "SentencePiece í† í¬ë‚˜ì´ì € vocab_size ì‹¤í—˜ : 10000, 15000, 20000, 30000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3caaae52-8e67-46ea-9941-6331694a11fd",
   "metadata": {},
   "source": [
    "\n",
    "âœ… SentencePiece í† í¬ë‚˜ì´ì € ì¤€ë¹„ ì™„ë£Œ (vocab_size: 10000)  \n",
    "Epoch 01 | Train Loss: 0.5399 | Train Acc: 70.49% | Val Loss: 0.3979 | Val Acc: 82.02%  \n",
    "Epoch 02 | Train Loss: 0.3511 | Train Acc: 84.81% | Val Loss: 0.3452 | Val Acc: 85.08%  \n",
    "Epoch 03 | Train Loss: 0.3024 | Train Acc: 87.21% | Val Loss: 0.3384 | Val Acc: 85.29%  \n",
    "Epoch 04 | Train Loss: 0.2738 | Train Acc: 88.66% | Val Loss: 0.3370 | Val Acc: 85.42%  \n",
    "Epoch 05 | Train Loss: 0.2450 | Train Acc: 90.09% | Val Loss: 0.3481 | Val Acc: 85.30%  \n",
    "Epoch 06 | Train Loss: 0.2126 | Train Acc: 91.61% | Val Loss: 0.3843 | Val Acc: 85.02%  \n",
    "Epoch 07 | Train Loss: 0.1756 | Train Acc: 93.37% | Val Loss: 0.3792 | Val Acc: 84.85%  \n",
    "ğŸ›‘ Early stopping triggered after 7 epochs.  \n",
    "  \n",
    "----------------------------------------  \n",
    "  - ìµœì¢… í…ŒìŠ¤íŠ¸ ì†ì‹¤ (Loss): 0.3418  \n",
    "  - ìµœì¢… í…ŒìŠ¤íŠ¸ ì •í™•ë„ (Accuracy): 85.39%  \n",
    "----------------------------------------  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bed8da-fd82-4527-8ca1-89f7b63e5a33",
   "metadata": {},
   "source": [
    "âœ… SentencePiece í† í¬ë‚˜ì´ì € ì¤€ë¹„ ì™„ë£Œ (vocab_size: 15000)  \n",
    "  \n",
    "Epoch 01 | Train Loss: 0.5547 | Train Acc: 69.23% | Val Loss: 0.4035 | Val Acc: 81.66%  \n",
    "Epoch 02 | Train Loss: 0.3527 | Train Acc: 84.83% | Val Loss: 0.3480 | Val Acc: 84.75%  \n",
    "Epoch 03 | Train Loss: 0.2962 | Train Acc: 87.69% | Val Loss: 0.3413 | Val Acc: 85.08%  \n",
    "Epoch 04 | Train Loss: 0.2642 | Train Acc: 89.23% | Val Loss: 0.3485 | Val Acc: 85.34%  \n",
    "Epoch 05 | Train Loss: 0.2351 | Train Acc: 90.60% | Val Loss: 0.3576 | Val Acc: 85.20%  \n",
    "Epoch 06 | Train Loss: 0.2016 | Train Acc: 92.17% | Val Loss: 0.3634 | Val Acc: 84.92%  \n",
    "ğŸ›‘ Early stopping triggered after 6 epochs. \n",
    " \n",
    "----------------------------------------  \n",
    "  - ìµœì¢… í…ŒìŠ¤íŠ¸ ì†ì‹¤ (Loss): 0.3462  \n",
    "  - ìµœì¢… í…ŒìŠ¤íŠ¸ ì •í™•ë„ (Accuracy): 85.05%  \n",
    "----------------------------------------  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1437d57-eb08-407e-8d17-b33350342fa1",
   "metadata": {},
   "source": [
    "âœ… SentencePiece í† í¬ë‚˜ì´ì € ì¤€ë¹„ ì™„ë£Œ (vocab_size: 20000)  \n",
    "  \n",
    "Epoch 01 | Train Loss: 0.5622 | Train Acc: 68.04% | Val Loss: 0.4033 | Val Acc: 81.60%  \n",
    "Epoch 02 | Train Loss: 0.3500 | Train Acc: 84.83% | Val Loss: 0.3519 | Val Acc: 84.70%  \n",
    "Epoch 03 | Train Loss: 0.2857 | Train Acc: 88.20% | Val Loss: 0.3471 | Val Acc: 85.05%  \n",
    "Epoch 04 | Train Loss: 0.2491 | Train Acc: 89.90% | Val Loss: 0.3487 | Val Acc: 84.98%  \n",
    "Epoch 05 | Train Loss: 0.2156 | Train Acc: 91.57% | Val Loss: 0.3740 | Val Acc: 84.95%  \n",
    "Epoch 06 | Train Loss: 0.1789 | Train Acc: 93.26% | Val Loss: 0.4074 | Val Acc: 84.33%  \n",
    "ğŸ›‘ Early stopping triggered after 6 epochs.  \n",
    "  \n",
    "----------------------------------------  \n",
    "  - ìµœì¢… í…ŒìŠ¤íŠ¸ ì†ì‹¤ (Loss): 0.3494  \n",
    "  - ìµœì¢… í…ŒìŠ¤íŠ¸ ì •í™•ë„ (Accuracy): 85.30%  \n",
    "----------------------------------------  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7592474-8fea-411e-8646-17c20e792ff8",
   "metadata": {},
   "source": [
    "âœ… SentencePiece í† í¬ë‚˜ì´ì € ì¤€ë¹„ ì™„ë£Œ (vocab_size: 30000)  \n",
    "  \n",
    "Epoch 01 | Train Loss: 0.5570 | Train Acc: 68.91% | Val Loss: 0.4129 | Val Acc: 80.99%  \n",
    "Epoch 02 | Train Loss: 0.3556 | Train Acc: 84.54% | Val Loss: 0.3513 | Val Acc: 84.43%  \n",
    "Epoch 03 | Train Loss: 0.2826 | Train Acc: 88.47% | Val Loss: 0.3419 | Val Acc: 84.96%  \n",
    "Epoch 04 | Train Loss: 0.2386 | Train Acc: 90.62% | Val Loss: 0.3507 | Val Acc: 85.07%  \n",
    "Epoch 05 | Train Loss: 0.1988 | Train Acc: 92.53% | Val Loss: 0.3736 | Val Acc: 84.61%  \n",
    "Epoch 06 | Train Loss: 0.1572 | Train Acc: 94.30% | Val Loss: 0.4311 | Val Acc: 84.42%  \n",
    "ğŸ›‘ Early stopping triggered after 6 epochs.  \n",
    "  \n",
    "----------------------------------------  \n",
    "  - ìµœì¢… í…ŒìŠ¤íŠ¸ ì†ì‹¤ (Loss): 0.3475  \n",
    "  - ìµœì¢… í…ŒìŠ¤íŠ¸ ì •í™•ë„ (Accuracy): 84.92%  \n",
    "----------------------------------------  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce477ba-87c6-4771-88dd-2f2556e6930d",
   "metadata": {},
   "source": [
    "KoNLPy(Mecab) í† í¬ë‚˜ì´ì € vocab_size ì‹¤í—˜ : 10000, 15000, 20000, 30000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee74eff4-9d96-4151-9bd9-1a38f1afc31f",
   "metadata": {},
   "source": [
    "âœ… KoNLPy(Mecab) í† í¬ë‚˜ì´ì € ì¤€ë¹„ ì™„ë£Œ (vocab_size: 10000)  \n",
    "  \n",
    "Epoch 01 | Train Loss: 0.4789 | Train Acc: 74.98% | Val Loss: 0.3603 | Val Acc: 83.92%  \n",
    "Epoch 02 | Train Loss: 0.3332 | Train Acc: 85.58% | Val Loss: 0.3225 | Val Acc: 85.93%  \n",
    "Epoch 03 | Train Loss: 0.2912 | Train Acc: 87.72% | Val Loss: 0.3114 | Val Acc: 86.64%  \n",
    "Epoch 04 | Train Loss: 0.2624 | Train Acc: 89.24% | Val Loss: 0.3087 | Val Acc: 86.55%  \n",
    "Epoch 05 | Train Loss: 0.2366 | Train Acc: 90.53% | Val Loss: 0.3193 | Val Acc: 86.60%  \n",
    "Epoch 06 | Train Loss: 0.2090 | Train Acc: 91.77% | Val Loss: 0.3393 | Val Acc: 86.44%  \n",
    "Epoch 07 | Train Loss: 0.1815 | Train Acc: 93.07% | Val Loss: 0.3482 | Val Acc: 86.04%  \n",
    "ğŸ›‘ Early stopping triggered after 7 epochs.  \n",
    "  \n",
    "----------------------------------------  \n",
    "  - ìµœì¢… í…ŒìŠ¤íŠ¸ ì†ì‹¤ (Loss): 0.3182  \n",
    "  - ìµœì¢… í…ŒìŠ¤íŠ¸ ì •í™•ë„ (Accuracy): 86.09%  \n",
    "----------------------------------------  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d8b543-bc08-44c2-a31a-a70a13a2dc1d",
   "metadata": {},
   "source": [
    "âœ… KoNLPy(Mecab) í† í¬ë‚˜ì´ì € ì¤€ë¹„ ì™„ë£Œ (vocab_size: 15000)  \n",
    "  \n",
    "Epoch 01 | Train Loss: 0.5170 | Train Acc: 71.30% | Val Loss: 0.3669 | Val Acc: 83.51%  \n",
    "Epoch 02 | Train Loss: 0.3343 | Train Acc: 85.60% | Val Loss: 0.3284 | Val Acc: 85.68%  \n",
    "Epoch 03 | Train Loss: 0.2896 | Train Acc: 87.92% | Val Loss: 0.3209 | Val Acc: 86.01%  \n",
    "Epoch 04 | Train Loss: 0.2581 | Train Acc: 89.50% | Val Loss: 0.3160 | Val Acc: 86.65%  \n",
    "Epoch 05 | Train Loss: 0.2299 | Train Acc: 90.81% | Val Loss: 0.3180 | Val Acc: 86.54%  \n",
    "Epoch 06 | Train Loss: 0.2021 | Train Acc: 92.21% | Val Loss: 0.3329 | Val Acc: 86.25%  \n",
    "Epoch 07 | Train Loss: 0.1731 | Train Acc: 93.52% | Val Loss: 0.3721 | Val Acc: 86.06%  \n",
    "ğŸ›‘ Early stopping triggered after 7 epochs.  \n",
    "  \n",
    "----------------------------------------  \n",
    "  - ìµœì¢… í…ŒìŠ¤íŠ¸ ì†ì‹¤ (Loss): 0.3249  \n",
    "  - ìµœì¢… í…ŒìŠ¤íŠ¸ ì •í™•ë„ (Accuracy): 86.30%  \n",
    "----------------------------------------  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986e04a5-1c3b-4688-8eab-fc238eb15fe4",
   "metadata": {},
   "source": [
    "âœ… KoNLPy(Mecab) í† í¬ë‚˜ì´ì € ì¤€ë¹„ ì™„ë£Œ (vocab_size: 20000)  \n",
    "  \n",
    "Epoch 01 | Train Loss: 0.4907 | Train Acc: 73.94% | Val Loss: 0.3606 | Val Acc: 83.89%  \n",
    "Epoch 02 | Train Loss: 0.3332 | Train Acc: 85.63% | Val Loss: 0.3277 | Val Acc: 85.88%  \n",
    "Epoch 03 | Train Loss: 0.2883 | Train Acc: 87.91% | Val Loss: 0.3155 | Val Acc: 86.53%  \n",
    "Epoch 04 | Train Loss: 0.2553 | Train Acc: 89.72% | Val Loss: 0.3141 | Val Acc: 86.85%  \n",
    "Epoch 05 | Train Loss: 0.2262 | Train Acc: 91.04% | Val Loss: 0.3241 | Val Acc: 86.67%  \n",
    "Epoch 06 | Train Loss: 0.1961 | Train Acc: 92.58% | Val Loss: 0.3430 | Val Acc: 86.58%  \n",
    "Epoch 07 | Train Loss: 0.1669 | Train Acc: 93.93% | Val Loss: 0.3711 | Val Acc: 86.40%  \n",
    "ğŸ›‘ Early stopping triggered after 7 epochs.  \n",
    "  \n",
    "----------------------------------------  \n",
    "  - ìµœì¢… í…ŒìŠ¤íŠ¸ ì†ì‹¤ (Loss): 0.3227  \n",
    "  - ìµœì¢… í…ŒìŠ¤íŠ¸ ì •í™•ë„ (Accuracy): 86.20%  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecefb74-73c7-474e-9c4b-1cf4af5fdb5c",
   "metadata": {},
   "source": [
    "âœ… KoNLPy(Mecab) í† í¬ë‚˜ì´ì € ì¤€ë¹„ ì™„ë£Œ (vocab_size: 30000)  \n",
    "  \n",
    "Epoch 01 | Train Loss: 0.4783 | Train Acc: 75.43% | Val Loss: 0.3633 | Val Acc: 83.89%  \n",
    "Epoch 02 | Train Loss: 0.3358 | Train Acc: 85.54% | Val Loss: 0.3286 | Val Acc: 85.63%  \n",
    "Epoch 03 | Train Loss: 0.2889 | Train Acc: 88.06% | Val Loss: 0.3157 | Val Acc: 86.34%  \n",
    "Epoch 04 | Train Loss: 0.2536 | Train Acc: 89.77% | Val Loss: 0.3076 | Val Acc: 86.91%  \n",
    "Epoch 05 | Train Loss: 0.2219 | Train Acc: 91.33% | Val Loss: 0.3161 | Val Acc: 86.73%  \n",
    "Epoch 06 | Train Loss: 0.1912 | Train Acc: 92.86% | Val Loss: 0.3369 | Val Acc: 86.48%  \n",
    "Epoch 07 | Train Loss: 0.1620 | Train Acc: 94.16% | Val Loss: 0.3642 | Val Acc: 86.30%  \n",
    "ğŸ›‘ Early stopping triggered after 7 epochs.  \n",
    "  \n",
    "----------------------------------------  \n",
    "  - ìµœì¢… í…ŒìŠ¤íŠ¸ ì†ì‹¤ (Loss): 0.3170  \n",
    "  - ìµœì¢… í…ŒìŠ¤íŠ¸ ì •í™•ë„ (Accuracy): 86.44%  \n",
    "----------------------------------------  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83417c48-323e-433b-9668-143bf9bd35b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7c3165-d71a-48ee-8975-f972384c8fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ë°ì´í„° ì¶”ê°€ ì „ì²˜ë¦¬ í…ŒìŠ¤íŠ¸\n",
    " #ë¶ˆìš©ì–´ ì¶”ê°€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb34bebb-27ec-4a2b-b44a-59286e33c719",
   "metadata": {},
   "source": [
    "# ====================================================================================\n",
    "# 4. ë°ì´í„° ì¤€ë¹„ í•¨ìˆ˜ (ìˆ˜ì •ë³¸)\n",
    "# ====================================================================================\n",
    "def load_and_preprocess_data():\n",
    "    \"\"\"NSMC ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ì‚¬ìš©ì ì •ì˜ ì „ì²˜ë¦¬ë¥¼ ì ìš©í•˜ëŠ” í•¨ìˆ˜\"\"\"\n",
    "    # 1. ë°ì´í„° ê²½ë¡œë¥¼ ì„¤ì •í•˜ê³  íŒŒì¼ì„ ì½ì–´ì˜µë‹ˆë‹¤.\n",
    "    data_path = os.path.join(os.getenv(\"HOME\"), 'work', 'workplace', 'AIFFEL_quest_rs', 'Exploration', 'Quest05', 'sentiment_classification', 'data')\n",
    "    train_data = pd.read_table(os.path.join(data_path, 'ratings_train.txt'))\n",
    "    test_data = pd.read_table(os.path.join(data_path, 'ratings_test.txt'))\n",
    "\n",
    "    # 2. --- í›ˆë ¨ ë°ì´í„° ê¸°ë³¸ ì •ì œ ---\n",
    "    train_data.dropna(subset=['document'], inplace=True) # 'document' ì—´ì— ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” í–‰ì„ ì œê±°í•©ë‹ˆë‹¤.\n",
    "    train_data.drop_duplicates(subset=['document'], inplace=True) # 'document' ì—´ì˜ ë‚´ìš©ì´ ì¤‘ë³µë˜ëŠ” í–‰ì„ ì œê±°í•©ë‹ˆë‹¤.\n",
    "\n",
    "    # 3. --- í…ŒìŠ¤íŠ¸ ë°ì´í„° ê¸°ë³¸ ì •ì œ ---\n",
    "    test_data.dropna(subset=['document'], inplace=True) # 'document' ì—´ì— ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” í–‰ì„ ì œê±°í•©ë‹ˆë‹¤.\n",
    "    test_data.drop_duplicates(subset=['document'], inplace=True) # 'document' ì—´ì˜ ë‚´ìš©ì´ ì¤‘ë³µë˜ëŠ” í–‰ì„ ì œê±°í•©ë‹ˆë‹¤.\n",
    "\n",
    "    # 4. --- ì‚¬ìš©ì ì •ì˜ ë¶ˆìš©ì–´ ì „ì²˜ë¦¬ (ì¶”ê°€ëœ ë¶€ë¶„) ---\n",
    "    # ë¶„ì„ì„ í†µí•´ ë„ì¶œí•œ, ê¸/ë¶€ì • íŒë‹¨ì— í˜¼ë€ì„ ì¤„ ìˆ˜ ìˆëŠ” ê³µí†µ ë‹¨ì–´ë¥¼ ë¶ˆìš©ì–´ë¡œ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "    CUSTOM_STOPWORDS = [\n",
    "        'ì˜í™”', 'ë„ˆë¬´', 'ì •ë§', 'ì§„ì§œ', 'ê·¸ëƒ¥', 'ë³´ê³ ', 'í•˜ëŠ”', 'ì´', 'ê·¸', 'ê²ƒ', 'ë˜',\n",
    "        'ë”', 'ìˆ˜', 'ì¢€', 'ì˜', 'ë‹¤', 'ë§', 'ì•ˆ', 'ë³¸', 'ë­', 'ì—†ëŠ”', 'ê°™ë‹¤', 'ì—†ë‹¤',\n",
    "        'ë´¤ëŠ”ë°', 'ì—°ê¸°', 'ë°°ìš°', 'ì ', 'ë‚´', 'ë‚œ', 'ì°¸', 'ì™œ', 'ë‹¤ì‹œ', 'ê°™ì€', 'ì™„ì „',\n",
    "        'ì •ë„', 'ê·¸ë˜ì„œ', 'ê·¸ë¦¬ê³ ', 'í•˜ì§€ë§Œ', 'ê·¼ë°', 'ì¼ë‹¨', 'ì˜¤ëŠ˜', 'ì—­ì‹œ', 'ë”±', 'íŠ¹íˆ'\n",
    "    ]\n",
    "\n",
    "    def remove_stopwords(text):\n",
    "        \"\"\"ì…ë ¥ëœ í…ìŠ¤íŠ¸ì—ì„œ CUSTOM_STOPWORDS ë¦¬ìŠ¤íŠ¸ì— í¬í•¨ëœ ë‹¨ì–´ë¥¼ ì œê±°í•˜ëŠ” í•¨ìˆ˜\"\"\"\n",
    "        if not isinstance(text, str): # ë§Œì•½ ì…ë ¥ê°’ì´ ë¬¸ìì—´ì´ ì•„ë‹ˆë¼ë©´\n",
    "            return '' # ë¹ˆ ë¬¸ìì—´ì„ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "        words = text.split(' ') # í…ìŠ¤íŠ¸ë¥¼ ê³µë°± ê¸°ì¤€ìœ¼ë¡œ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ë¡œ ë¶„ë¦¬í•©ë‹ˆë‹¤.\n",
    "        filtered_words = [word for word in words if word not in CUSTOM_STOPWORDS] # ë¶ˆìš©ì–´ì— í¬í•¨ë˜ì§€ ì•ŠëŠ” ë‹¨ì–´ë§Œ ì„ íƒí•©ë‹ˆë‹¤.\n",
    "        return ' '.join(filtered_words) # í•„í„°ë§ëœ ë‹¨ì–´ë“¤ì„ ë‹¤ì‹œ ê³µë°±ìœ¼ë¡œ ì—°ê²°í•˜ì—¬ ë¬¸ì¥ìœ¼ë¡œ ë§Œë“­ë‹ˆë‹¤.\n",
    "\n",
    "    print(\"â³ ì‚¬ìš©ì ì •ì˜ ë¶ˆìš©ì–´ ì œê±°ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
    "    # í›ˆë ¨ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ 'document' ì—´ì— ë¶ˆìš©ì–´ ì œê±° í•¨ìˆ˜ë¥¼ ì ìš©í•©ë‹ˆë‹¤.\n",
    "    train_data['document'] = train_data['document'].apply(remove_stopwords)\n",
    "    test_data['document'] = test_data['document'].apply(remove_stopwords)\n",
    "    \n",
    "    # (ì„ íƒ) ë¶ˆìš©ì–´ ì œê±° í›„ ë‚´ìš©ì´ ì™„ì „íˆ ë¹„ì–´ë²„ë¦° í–‰ì´ ìˆë‹¤ë©´ ì œê±°í•©ë‹ˆë‹¤.\n",
    "    train_data = train_data[train_data['document'] != '']\n",
    "    test_data = test_data[test_data['document'] != '']\n",
    "    print(\"âœ… ì‚¬ìš©ì ì •ì˜ ë¶ˆìš©ì–´ ì œê±° ì™„ë£Œ.\")\n",
    "\n",
    "    # 5. --- ë°ì´í„°ì…‹ ë¶„ë¦¬ ---\n",
    "    # ì „ì²˜ë¦¬ëœ í›ˆë ¨ ë°ì´í„°ë¥¼ í›ˆë ¨ìš©ê³¼ ê²€ì¦ìš©ìœ¼ë¡œ 8:2 ë¹„ìœ¨ë¡œ ë¶„ë¦¬í•©ë‹ˆë‹¤.\n",
    "    train_set, val_set = train_test_split(train_data, test_size=0.2, random_state=CFG['SEED'], stratify=train_data['label'])\n",
    "\n",
    "    print(\"âœ… ë°ì´í„° ë¡œë“œ ë° ëª¨ë“  ì „ì²˜ë¦¬ ì™„ë£Œ\")\n",
    "    return train_set, val_set, test_data # ìµœì¢…ì ìœ¼ë¡œ ë¶„ë¦¬ëœ ë°ì´í„°ì…‹ë“¤ì„ ë°˜í™˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47189659-796e-4a45-81f2-34c6ec25ded4",
   "metadata": {},
   "source": [
    "âœ… ì‚¬ìš©ì ì •ì˜ ë¶ˆìš©ì–´ ì œê±° ì™„ë£Œ.  \n",
    "âœ… SentencePiece í† í¬ë‚˜ì´ì € ì¤€ë¹„ ì™„ë£Œ (vocab_size: 10000)  \n",
    "  \n",
    "Epoch 01 | Train Loss: 0.5448 | Train Acc: 69.99% | Val Loss: 0.4047 | Val Acc: 81.70%   \n",
    "Epoch 02 | Train Loss: 0.3523 | Train Acc: 84.70% | Val Loss: 0.3566 | Val Acc: 84.33%  \n",
    "Epoch 03 | Train Loss: 0.3049 | Train Acc: 87.07% | Val Loss: 0.3517 | Val Acc: 84.85%  \n",
    "Epoch 04 | Train Loss: 0.2768 | Train Acc: 88.52% | Val Loss: 0.3641 | Val Acc: 84.65%  \n",
    "Epoch 05 | Train Loss: 0.2467 | Train Acc: 89.85% | Val Loss: 0.3641 | Val Acc: 84.67%  \n",
    "Epoch 06 | Train Loss: 0.2134 | Train Acc: 91.52% | Val Loss: 0.3836 | Val Acc: 83.96%  \n",
    "ğŸ›‘ Early stopping triggered after 6 epochs.  \n",
    "  \n",
    "----------------------------------------  \n",
    "  - ìµœì¢… í…ŒìŠ¤íŠ¸ ì†ì‹¤ (Loss): 0.3505  \n",
    "  - ìµœì¢… í…ŒìŠ¤íŠ¸ ì •í™•ë„ (Accuracy): 84.92%  \n",
    "----------------------------------------  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbc1d1c-2342-4257-a5fc-15d9beba58c9",
   "metadata": {},
   "source": [
    "âœ… ì‚¬ìš©ì ì •ì˜ ë¶ˆìš©ì–´ ì œê±° ì™„ë£Œ.  \n",
    "âœ… KoNLPy(Mecab) í† í¬ë‚˜ì´ì € ì¤€ë¹„ ì™„ë£Œ (vocab_size: 10000)  \n",
    "  \n",
    "Epoch 01 | Train Loss: 0.4803 | Train Acc: 75.03% | Val Loss: 0.3707 | Val Acc: 83.11%  \n",
    "Epoch 02 | Train Loss: 0.3366 | Train Acc: 85.24% | Val Loss: 0.3404 | Val Acc: 85.10%  \n",
    "Epoch 03 | Train Loss: 0.2942 | Train Acc: 87.46% | Val Loss: 0.3342 | Val Acc: 85.56%  \n",
    "Epoch 04 | Train Loss: 0.2658 | Train Acc: 88.98% | Val Loss: 0.3267 | Val Acc: 85.85%  \n",
    "Epoch 05 | Train Loss: 0.2397 | Train Acc: 90.23% | Val Loss: 0.3350 | Val Acc: 85.65%  \n",
    "Epoch 06 | Train Loss: 0.2128 | Train Acc: 91.53% | Val Loss: 0.3497 | Val Acc: 85.32%  \n",
    "Epoch 07 | Train Loss: 0.1854 | Train Acc: 92.92% | Val Loss: 0.3707 | Val Acc: 85.73%  \n",
    "ğŸ›‘ Early stopping triggered after 7 epochs.  \n",
    "  \n",
    "----------------------------------------  \n",
    "  - ìµœì¢… í…ŒìŠ¤íŠ¸ ì†ì‹¤ (Loss): 0.3267  \n",
    "  - ìµœì¢… í…ŒìŠ¤íŠ¸ ì •í™•ë„ (Accuracy): 85.86%  \n",
    "----------------------------------------  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d10c38-691a-46b6-a92d-8b3756075839",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ba8512-476a-4faa-8fcf-d6f479dcdcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentencepiece í† í¬ë‚˜ì´ì € ëª¨ë¸ ë³€ê²½ bpe > unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60623231-bcce-4f13-a1a9-3c9e0c4037c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# 2. ì¤‘ì•™ ì„¤ì • (Configuration)\n",
    "# ====================================================================================\n",
    "CFG = {\n",
    "    # --- ì‹¤í—˜ ì„ íƒ ---\n",
    "    'TOKENIZER_TYPE': 'SentencePiece', # 'SentencePiece' ë˜ëŠ” 'KoNLPy' ì„ íƒ\n",
    "    'KONLPY_TOKENIZER': 'Mecab',     # TOKENIZER_TYPEì´ 'KoNLPy'ì¼ ê²½ìš°, ì‚¬ìš©í•  í˜•íƒœì†Œ ë¶„ì„ê¸°\n",
    "    \n",
    "    # --- SentencePiece í•˜ì´í¼íŒŒë¼ë¯¸í„° ---\n",
    "    'SP_VOCAB_SIZE': 10000,          # SentencePiece ë‹¨ì–´ ì§‘í•© í¬ê¸°\n",
    "    'SP_MODEL_TYPE': 'unigram',          # 'bpe' ë˜ëŠ” 'unigram'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a6190d-2dbd-4742-b508-313220d182a7",
   "metadata": {},
   "source": [
    "âœ… 'SP_MODEL_TYPE': 'unigram' í† í¬ë‚˜ì´ì € ì¤€ë¹„ ì™„ë£Œ (vocab_size: 10000)  \n",
    "  \n",
    "Epoch 01 | Train Loss: 0.5444 | Train Acc: 69.63% | Val Loss: 0.3956 | Val Acc: 82.28%  \n",
    "Epoch 02 | Train Loss: 0.3510 | Train Acc: 84.78% | Val Loss: 0.3459 | Val Acc: 84.75%  \n",
    "Epoch 03 | Train Loss: 0.3039 | Train Acc: 87.08% | Val Loss: 0.3380 | Val Acc: 85.38%  \n",
    "Epoch 04 | Train Loss: 0.2771 | Train Acc: 88.41% | Val Loss: 0.3371 | Val Acc: 85.41%  \n",
    "Epoch 05 | Train Loss: 0.2501 | Train Acc: 89.77% | Val Loss: 0.3464 | Val Acc: 85.43%  \n",
    "Epoch 06 | Train Loss: 0.2195 | Train Acc: 91.24% | Val Loss: 0.3649 | Val Acc: 85.16%  \n",
    "Epoch 07 | Train Loss: 0.1855 | Train Acc: 92.84% | Val Loss: 0.4018 | Val Acc: 85.00%  \n",
    "ğŸ›‘ Early stopping triggered after 7 epochs.  \n",
    "  \n",
    "----------------------------------------  \n",
    "  - ìµœì¢… í…ŒìŠ¤íŠ¸ ì†ì‹¤ (Loss): 0.3421  \n",
    "  - ìµœì¢… í…ŒìŠ¤íŠ¸ ì •í™•ë„ (Accuracy): 85.19%  \n",
    "----------------------------------------  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e153f62d-304d-464d-8bff-83dd69931869",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93ba3d3-d877-439e-9775-5172ab80d2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì˜µì…˜ì´ ì¶”ê°€ëœ SentencePiece í† í¬ë‚˜ì´ì € ì‹¤í—˜\n",
    "    #character_coverage, user_defined_symbols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4735674-343c-490e-92ad-08d39eab7292",
   "metadata": {},
   "source": [
    "-character_coverage\t1.0\t'ã…‹', '!', 'â™¡' ë“± ëª¨ë“  ê¸°í˜¸ì™€ ì´ëª¨í‹°ì½˜ì„ ë‹¨ì–´ ì‚¬ì „ì— í¬í•¨ì‹œí‚¤ê¸° ìœ„í•¨ì…ë‹ˆë‹¤. \n",
    "ê¸°ë³¸ê°’(0.9995)ì€ ë“œë¬¼ê²Œ ë‚˜íƒ€ë‚˜ëŠ” ë¬¸ìë¥¼ ì œì™¸í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ 1.0ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ëª¨ë“  ì •ë³´ë¥¼ ë³´ì¡´í•©ë‹ˆë‹¤.  \n",
    "   \n",
    "-user_defined_symbols\tã…‹ã…‹, ã…ã…, ã… ã… , ã…¡ã…¡, !, ?\të“± ì˜µì…˜ì— ìì£¼ ì“°ì´ëŠ” ê¸°í˜¸ë“¤ì„ ë“±ë¡í•˜ë©´, \n",
    "SentencePieceê°€ ì´ë“¤ì„ í•˜ë‚˜ì˜ ì˜ë¯¸ ìˆëŠ” ë‹¨ìœ„(í† í°)ë¡œ ì·¨ê¸‰í•  ìˆ˜ ìˆìŒ. \n",
    "'ã…‹ã…‹'ê°€ 'ã…‹', 'ã…‹'ë¡œ ë¶„ë¦¬ë˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ì—¬ ê°ì • ì‹ í˜¸ë¥¼ ê·¸ëŒ€ë¡œ ìœ ì§€í•  ìˆ˜ ìˆìŒ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437f97f5-83a7-49e9-ba9d-e7f97652e5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# 5. í† í¬ë‚˜ì´ì € ìƒì„± í•¨ìˆ˜ (KoNLPy ë¡œì§ ìˆ˜ì •)\n",
    "# ====================================================================================\n",
    "# get_tokenizer_modified\n",
    "def get_tokenizer(cfg, train_df):\n",
    "    \"\"\"CFGì— ë”°ë¼ SentencePiece ë˜ëŠ” KoNLPy í† í¬ë‚˜ì´ì €ì™€ vocab_sizeë¥¼ ë°˜í™˜\"\"\"\n",
    "\n",
    "    if cfg['TOKENIZER_TYPE'] == 'SentencePiece':\n",
    "        # --- (SentencePiece ë¡œì§ ìˆ˜ì •) ---\n",
    "        corpus_path = 'nsmc_corpus.txt' # í›ˆë ¨ ë°ì´í„°ë¡œ ì‚¬ìš©í•  í…ìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œì…ë‹ˆë‹¤.\n",
    "        model_prefix = f'nsmc_{cfg[\"SP_MODEL_TYPE\"]}_{cfg[\"SP_VOCAB_SIZE\"]}' # ìƒì„±ë  SentencePiece ëª¨ë¸ íŒŒì¼ì˜ ì ‘ë‘ì‚¬ì…ë‹ˆë‹¤.\n",
    "        train_df['document'].to_csv(corpus_path, index=False, header=False) # í›ˆë ¨ ë°ì´í„°ë¥¼ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "        \n",
    "        # <<<--- ìˆ˜ì •ëœ ë¶€ë¶„ START ---\n",
    "        \n",
    "        # 1. user_defined_symbols ì˜µì…˜ì— ì‚¬ìš©í•  ê¸°í˜¸ë“¤ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "        user_symbols = ['ã…‹ã…‹', 'ã…ã…', 'ã… ã… ', 'ã…¡ã…¡', '!', '?']\n",
    "        # 2. ë¦¬ìŠ¤íŠ¸ë¥¼ ì‰¼í‘œ(,)ë¡œ êµ¬ë¶„ëœ ë¬¸ìì—´ í˜•íƒœë¡œ ë³€í™˜í•©ë‹ˆë‹¤. (ex: 'ã…‹ã…‹,ã…ã…,ã… ã… ,...')\n",
    "        user_symbols_str = ','.join(user_symbols)\n",
    "\n",
    "        # 3. SentencePieceTrainer.trainì— ìƒˆë¡œìš´ ì˜µì…˜ì„ ì¶”ê°€í•©ë‹ˆë‹¤.\n",
    "        spm.SentencePieceTrainer.train(\n",
    "            f'--input={corpus_path} --model_prefix={model_prefix} '\n",
    "            f'--vocab_size={cfg[\"SP_VOCAB_SIZE\"]} --model_type={cfg[\"SP_MODEL_TYPE\"]} '\n",
    "            f'--character_coverage=1.0 ' # ëª¨ë“  ë¬¸ìë¥¼ ë‹¨ì–´ ì‚¬ì „ì— í¬í•¨ì‹œí‚µë‹ˆë‹¤.\n",
    "            f'--user_defined_symbols={user_symbols_str}' # ì‚¬ìš©ìê°€ ì •ì˜í•œ ê¸°í˜¸ë¥¼ í•˜ë‚˜ì˜ í† í°ìœ¼ë¡œ ì·¨ê¸‰í•©ë‹ˆë‹¤.\n",
    "        )\n",
    "        # <<<--- ìˆ˜ì •ëœ ë¶€ë¶„ END ---\n",
    "\n",
    "        processor = spm.SentencePieceProcessor() # í•™ìŠµëœ ëª¨ë¸ì„ ë¡œë“œí•  í”„ë¡œì„¸ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "        processor.load(f'{model_prefix}.model') # í•™ìŠµëœ SentencePiece ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "        vocab_size = processor.get_piece_size() # ìµœì¢… ë‹¨ì–´ ì§‘í•©ì˜ í¬ê¸°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
    "\n",
    "        def tokenize_fn(corpus, max_len):\n",
    "            \"\"\"ì…ë ¥ëœ ë¬¸ì¥(corpus)ì„ í† í°í™”í•˜ê³  íŒ¨ë”©í•˜ëŠ” ë‚´ë¶€ í•¨ìˆ˜\"\"\"\n",
    "            sequences = [] # í† í°í™”ëœ ì‹œí€€ìŠ¤ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.\n",
    "            for sentence in corpus: # ê° ë¬¸ì¥ì— ëŒ€í•´ ë°˜ë³µí•©ë‹ˆë‹¤.\n",
    "                ids = processor.encode_as_ids(str(sentence)) # ë¬¸ì¥ì„ ì •ìˆ˜ ID ì‹œí€€ìŠ¤ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "                # ë¬¸ì¥ì˜ ê¸¸ì´ë¥¼ max_lenì— ë§ê²Œ ìë¥´ê±°ë‚˜, ë¶€ì¡±í•œ ë¶€ë¶„ì„ 0ìœ¼ë¡œ ì±„ì›ë‹ˆë‹¤(íŒ¨ë”©).\n",
    "                ids = ids[:max_len] if len(ids) > max_len else ids + [0] * (max_len - len(ids))\n",
    "                sequences.append(ids) # ì²˜ë¦¬ëœ ì‹œí€€ìŠ¤ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€í•©ë‹ˆë‹¤.\n",
    "            return torch.tensor(sequences, dtype=torch.long) # ìµœì¢… ê²°ê³¼ë¥¼ PyTorch í…ì„œë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "            \n",
    "        print(f\"âœ… SentencePiece í† í¬ë‚˜ì´ì € ì¤€ë¹„ ì™„ë£Œ (vocab_size: {vocab_size})\")\n",
    "        return tokenize_fn, vocab_size # í† í°í™” í•¨ìˆ˜ì™€ ë‹¨ì–´ ì§‘í•© í¬ê¸°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "\n",
    "    elif cfg['TOKENIZER_TYPE'] == 'KoNLPy':\n",
    "        # --- (KoNLPy ë¡œì§ì€ ì´ì „ê³¼ ë™ì¼) ---\n",
    "        if cfg['KONLPY_TOKENIZER'] == 'Mecab':\n",
    "            tokenizer = Mecab('/usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ko-dic')\n",
    "        else:\n",
    "            from konlpy.tag import Okt\n",
    "            tokenizer = Okt()\n",
    "            \n",
    "        word_counts = {}\n",
    "        for sentence in tqdm(train_df['document'], desc=\"KoNLPy Freq. Counting\"):\n",
    "            tokens = tokenizer.morphs(str(sentence))\n",
    "            for token in tokens:\n",
    "                word_counts[token] = word_counts.get(token, 0) + 1\n",
    "        \n",
    "        sorted_words = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "        top_words = sorted_words[:cfg['KONLPY_VOCAB_SIZE'] - 2]\n",
    "        \n",
    "        word_index = {'<PAD>': 0, '<UNK>': 1}\n",
    "        for word in top_words:\n",
    "            word_index[word] = len(word_index)\n",
    "            \n",
    "        vocab_size = len(word_index)\n",
    "\n",
    "        def tokenize_fn(corpus, max_len):\n",
    "            sequences = []\n",
    "            for sentence in corpus:\n",
    "                tokens = tokenizer.morphs(str(sentence))\n",
    "                ids = [word_index.get(token, 1) for token in tokens] \n",
    "                ids = ids[:max_len] if len(ids) > max_len else ids + [0] * (max_len - len(ids))\n",
    "                sequences.append(ids)\n",
    "            return torch.tensor(sequences, dtype=torch.long)\n",
    "            \n",
    "        print(f\"âœ… KoNLPy({cfg['KONLPY_TOKENIZER']}) í† í¬ë‚˜ì´ì € ì¤€ë¹„ ì™„ë£Œ (vocab_size: {vocab_size})\")\n",
    "        return tokenize_fn, vocab_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c78fa1-26ba-4663-a747-2ef39f394562",
   "metadata": {},
   "source": [
    "âœ… SentencePiece í† í¬ë‚˜ì´ì € ì¤€ë¹„ ì™„ë£Œ (vocab_size: 10000)  \n",
    "     -character_coverage, user_defined_symbols ìˆ˜ì •  \n",
    "       \n",
    "Epoch 01 | Train Loss: 0.5392 | Train Acc: 70.69% | Val Loss: 0.3970 | Val Acc: 82.17%  \n",
    "Epoch 02 | Train Loss: 0.3524 | Train Acc: 84.78% | Val Loss: 0.3490 | Val Acc: 84.84%  \n",
    "Epoch 03 | Train Loss: 0.3069 | Train Acc: 86.92% | Val Loss: 0.3369 | Val Acc: 85.31%  \n",
    "Epoch 04 | Train Loss: 0.2816 | Train Acc: 88.19% | Val Loss: 0.3335 | Val Acc: 85.55%  \n",
    "Epoch 05 | Train Loss: 0.2558 | Train Acc: 89.44% | Val Loss: 0.3449 | Val Acc: 85.40%  \n",
    "Epoch 06 | Train Loss: 0.2266 | Train Acc: 90.80% | Val Loss: 0.3592 | Val Acc: 85.11%  \n",
    "Epoch 07 | Train Loss: 0.1932 | Train Acc: 92.43% | Val Loss: 0.4015 | Val Acc: 84.96%  \n",
    "ğŸ›‘ Early stopping triggered after 7 epochs.  \n",
    "  \n",
    "----------------------------------------  \n",
    "  - ìµœì¢… í…ŒìŠ¤íŠ¸ ì†ì‹¤ (Loss): 0.3390  \n",
    "  - ìµœì¢… í…ŒìŠ¤íŠ¸ ì •í™•ë„ (Accuracy): 85.19%  \n",
    "----------------------------------------  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6470cdd0-dcf3-4689-b92d-8fe09ee7e577",
   "metadata": {},
   "source": [
    "ìµœëŒ€ ê¸¸ì´ 40 -> 110  \n",
    "âœ… SentencePiece í† í¬ë‚˜ì´ì € ì¤€ë¹„ ì™„ë£Œ (vocab_size: 10000)  \n",
    "  \n",
    "Epoch 01 | Train Loss: 0.5775 | Train Acc: 67.12% | Val Loss: 0.4165 | Val Acc: 81.40%  \n",
    "Epoch 02 | Train Loss: 0.3623 | Train Acc: 84.48% | Val Loss: 0.3483 | Val Acc: 85.01%  \n",
    "Epoch 03 | Train Loss: 0.3029 | Train Acc: 87.26% | Val Loss: 0.3360 | Val Acc: 85.52%  \n",
    "Epoch 04 | Train Loss: 0.2720 | Train Acc: 88.86% | Val Loss: 0.3403 | Val Acc: 85.62%   \n",
    "Epoch 05 | Train Loss: 0.2409 | Train Acc: 90.31% | Val Loss: 0.3518 | Val Acc: 85.33%  \n",
    "Epoch 06 | Train Loss: 0.2068 | Train Acc: 91.84% | Val Loss: 0.3768 | Val Acc: 85.09%  \n",
    "ğŸ›‘ Early stopping triggered after 6 epochs.  \n",
    "  \n",
    "----------------------------------------  \n",
    "  - ìµœì¢… í…ŒìŠ¤íŠ¸ ì†ì‹¤ (Loss): 0.3420  \n",
    "  - ìµœì¢… í…ŒìŠ¤íŠ¸ ì •í™•ë„ (Accuracy): 85.29%  \n",
    "----------------------------------------  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1117ec27-51d5-4385-b1cd-dc3b433a6ec2",
   "metadata": {},
   "source": [
    "ìµœëŒ€ ê¸¸ì´ 40 -> 110  \n",
    "âœ… KoNLPy(Mecab) í† í¬ë‚˜ì´ì € ì¤€ë¹„ ì™„ë£Œ (vocab_size: 10000)  \n",
    " \n",
    "Epoch 01 | Train Loss: 0.4902 | Train Acc: 74.49% | Val Loss: 0.3662 | Val Acc: 83.49%  \n",
    "Epoch 02 | Train Loss: 0.3320 | Train Acc: 85.73% | Val Loss: 0.3188 | Val Acc: 86.05%  \n",
    "Epoch 03 | Train Loss: 0.2858 | Train Acc: 88.03% | Val Loss: 0.3071 | Val Acc: 86.89%  \n",
    "Epoch 04 | Train Loss: 0.2563 | Train Acc: 89.50% | Val Loss: 0.3076 | Val Acc: 86.98%  \n",
    "Epoch 05 | Train Loss: 0.2298 | Train Acc: 90.77% | Val Loss: 0.3121 | Val Acc: 87.16%  \n",
    "Epoch 06 | Train Loss: 0.2022 | Train Acc: 91.95% | Val Loss: 0.3412 | Val Acc: 86.88%  \n",
    "ğŸ›‘ Early stopping triggered after 6 epochs.  \n",
    "   \n",
    "----------------------------------------  \n",
    "  - ìµœì¢… í…ŒìŠ¤íŠ¸ ì†ì‹¤ (Loss): 0.3159  \n",
    "  - ìµœì¢… í…ŒìŠ¤íŠ¸ ì •í™•ë„ (Accuracy): 86.37%  \n",
    "----------------------------------------  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5cf4db-0d97-45f1-96a5-62165a2c640a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71a5f77c-023d-40ad-9d5e-bbf836b52ffc",
   "metadata": {},
   "source": [
    "KoNLPy í’ˆì‚¬(POS) í•„í„°ë§ì´ ì ìš©ëœ í† í¬ë‚˜ì´ì € ìƒì„±-í˜•ìš©ì‚¬ ê°•ì¡°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7efa45-eef7-4583-8ccf-3f9046dc619c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# 5. í† í¬ë‚˜ì´ì € ìƒì„± í•¨ìˆ˜ (KoNLPy ë¡œì§ ìˆ˜ì •)\n",
    "# ====================================================================================\n",
    "def get_tokenizer(cfg, train_df):\n",
    "    \"\"\"CFGì— ë”°ë¼ SentencePiece ë˜ëŠ” KoNLPy í† í¬ë‚˜ì´ì €ì™€ vocab_sizeë¥¼ ë°˜í™˜\"\"\"\n",
    "    \n",
    "    if cfg['TOKENIZER_TYPE'] == 'SentencePiece':\n",
    "        # --- (SentencePiece ë¡œì§ì€ ì´ì „ê³¼ ë™ì¼) ---\n",
    "        corpus_path = 'nsmc_corpus.txt'\n",
    "        model_prefix = f'nsmc_{cfg[\"SP_MODEL_TYPE\"]}_{cfg[\"SP_VOCAB_SIZE\"]}'\n",
    "        train_df['document'].to_csv(corpus_path, index=False, header=False)\n",
    "        spm.SentencePieceTrainer.train(\n",
    "            f'--input={corpus_path} --model_prefix={model_prefix} '\n",
    "            f'--vocab_size={cfg[\"SP_VOCAB_SIZE\"]} --model_type={cfg[\"SP_MODEL_TYPE\"]}'\n",
    "        )\n",
    "        processor = spm.SentencePieceProcessor()\n",
    "        processor.load(f'{model_prefix}.model')\n",
    "        vocab_size = processor.get_piece_size()\n",
    "\n",
    "        def tokenize_fn(corpus, max_len):\n",
    "            sequences = []\n",
    "            for sentence in corpus:\n",
    "                ids = processor.encode_as_ids(str(sentence))\n",
    "                ids = ids[:max_len] if len(ids) > max_len else ids + [0] * (max_len - len(ids))\n",
    "                sequences.append(ids)\n",
    "            return torch.tensor(sequences, dtype=torch.long)\n",
    "            \n",
    "        print(f\"âœ… SentencePiece í† í¬ë‚˜ì´ì € ì¤€ë¹„ ì™„ë£Œ (vocab_size: {vocab_size})\")\n",
    "        return tokenize_fn, vocab_size\n",
    "\n",
    "    elif cfg['TOKENIZER_TYPE'] == 'KoNLPy':\n",
    "        # --- KoNLPy í† í¬ë‚˜ì´ì € ìƒì„± (POS í•„í„°ë§ ë¡œì§ ì¶”ê°€) ---\n",
    "        if cfg['KONLPY_TOKENIZER'] == 'Mecab':\n",
    "            tokenizer = Mecab('/usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ko-dic')\n",
    "        else:\n",
    "            from konlpy.tag import Okt\n",
    "            tokenizer = Okt()\n",
    "\n",
    "        # <<<--- ìˆ˜ì •ëœ ë¶€ë¶„ START ---\n",
    "\n",
    "        # 1. ì–´íœ˜ ì‚¬ì „ì— í¬í•¨í•  í’ˆì‚¬(POS) íƒœê·¸ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "        # Mecab í’ˆì‚¬ íƒœê·¸ ê¸°ì¤€: NNG(ì¼ë°˜ ëª…ì‚¬), NNP(ê³ ìœ  ëª…ì‚¬), VV(ë™ì‚¬), VA(í˜•ìš©ì‚¬)\n",
    "        TARGET_POS = ['NNG', 'NNP', 'VV', 'VA']\n",
    "        \n",
    "        # 2. ë‹¨ì–´ ë¹ˆë„ìˆ˜ ê³„ì‚° ì‹œ, ì •ì˜ëœ í’ˆì‚¬ì— í•´ë‹¹í•˜ëŠ” ë‹¨ì–´ë§Œ ì¹´ìš´íŠ¸í•©ë‹ˆë‹¤.\n",
    "        word_counts = {}\n",
    "        print(\"â³ KoNLPy ë‹¨ì–´ ë¹ˆë„ ê³„ì‚° ì¤‘ (POS í•„í„°ë§ ì ìš©)...\")\n",
    "        for sentence in tqdm(train_df['document']):\n",
    "            # tokenizer.pos()ë¥¼ ì‚¬ìš©í•˜ì—¬ (ë‹¨ì–´, í’ˆì‚¬) íŠœí”Œì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜ë°›ìŠµë‹ˆë‹¤.\n",
    "            pos_tokens = tokenizer.pos(str(sentence))\n",
    "            # TARGET_POS ë¦¬ìŠ¤íŠ¸ì— í¬í•¨ëœ í’ˆì‚¬ë¥¼ ê°€ì§„ ë‹¨ì–´(í† í°)ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.\n",
    "            tokens = [token for token, pos in pos_tokens if pos in TARGET_POS]\n",
    "            for token in tokens: # í•„í„°ë§ëœ ë‹¨ì–´ë“¤ì— ëŒ€í•´ì„œë§Œ ë¹ˆë„ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
    "                word_counts[token] = word_counts.get(token, 0) + 1\n",
    "        \n",
    "        # 3. ë¹ˆë„ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ìƒìœ„ ë‹¨ì–´ ì„ íƒ (ê¸°ì¡´ê³¼ ë™ì¼)\n",
    "        sorted_words = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "        top_words = sorted_words[:cfg['KONLPY_VOCAB_SIZE'] - 2]\n",
    "        \n",
    "        # 4. ìµœì¢… ë‹¨ì–´ ì‚¬ì „ êµ¬ì¶• (ê¸°ì¡´ê³¼ ë™ì¼)\n",
    "        word_index = {'<PAD>': 0, '<UNK>': 1}\n",
    "        for word in top_words:\n",
    "            word_index[word] = len(word_index)\n",
    "            \n",
    "        vocab_size = len(word_index)\n",
    "\n",
    "        def tokenize_fn(corpus, max_len):\n",
    "            \"\"\"ì…ë ¥ëœ ë¬¸ì¥ì„ í† í°í™”í•˜ê³  íŒ¨ë”©í•˜ëŠ” ë‚´ë¶€ í•¨ìˆ˜ (POS í•„í„°ë§ ì ìš©)\"\"\"\n",
    "            sequences = []\n",
    "            for sentence in corpus:\n",
    "                # 5. í† í°í™” ì‹œì—ë„ ì–´íœ˜ ì‚¬ì „ì„ ë§Œë“¤ ë•Œì™€ ë™ì¼í•œ í’ˆì‚¬ í•„í„°ë§ì„ ì ìš©í•©ë‹ˆë‹¤.\n",
    "                pos_tokens = tokenizer.pos(str(sentence))\n",
    "                tokens = [token for token, pos in pos_tokens if pos in TARGET_POS]\n",
    "                ids = [word_index.get(token, 1) for token in tokens] # ì‚¬ì „ì— ì—†ìœ¼ë©´ <UNK> (1)\n",
    "                ids = ids[:max_len] if len(ids) > max_len else ids + [0] * (max_len - len(ids))\n",
    "                sequences.append(ids)\n",
    "            return torch.tensor(sequences, dtype=torch.long)\n",
    "        \n",
    "        # <<<--- ìˆ˜ì •ëœ ë¶€ë¶„ END ---\n",
    "            \n",
    "        print(f\"âœ… KoNLPy({cfg['KONLPY_TOKENIZER']}) í† í¬ë‚˜ì´ì € ì¤€ë¹„ ì™„ë£Œ (vocab_size: {vocab_size})\")\n",
    "        return tokenize_fn, vocab_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b18ee86-24cc-4985-a10d-b9b3fcb111f1",
   "metadata": {},
   "source": [
    "âœ… KoNLPy(Mecab) í† í¬ë‚˜ì´ì € ì¤€ë¹„ ì™„ë£Œ (vocab_size: 31038)  \n",
    "  \n",
    "Epoch 01 | Train Loss: 0.5346 | Train Acc: 71.21% | Val Loss: 0.4610 | Val Acc: 77.38%  \n",
    "Epoch 02 | Train Loss: 0.4332 | Train Acc: 79.67% | Val Loss: 0.4375 | Val Acc: 79.18%  \n",
    "Epoch 03 | Train Loss: 0.3960 | Train Acc: 82.08% | Val Loss: 0.4308 | Val Acc: 79.62%  \n",
    "Epoch 04 | Train Loss: 0.3678 | Train Acc: 83.85% | Val Loss: 0.4304 | Val Acc: 79.76%  \n",
    "Epoch 05 | Train Loss: 0.3385 | Train Acc: 85.40% | Val Loss: 0.4433 | Val Acc: 79.69%  \n",
    "Epoch 06 | Train Loss: 0.3075 | Train Acc: 87.12% | Val Loss: 0.4724 | Val Acc: 79.01%  \n",
    "Epoch 07 | Train Loss: 0.2743 | Train Acc: 88.72% | Val Loss: 0.4935 | Val Acc: 78.64%  \n",
    "ğŸ›‘ Early stopping triggered after 7 epochs.  \n",
    "  \n",
    "----------------------------------------  \n",
    "  - ìµœì¢… í…ŒìŠ¤íŠ¸ ì†ì‹¤ (Loss): 0.4281   \n",
    "  - ìµœì¢… í…ŒìŠ¤íŠ¸ ì •í™•ë„ (Accuracy): 79.81%  \n",
    "----------------------------------------  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1d91f6-1d77-4c48-9bdd-a677cb5ec474",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
